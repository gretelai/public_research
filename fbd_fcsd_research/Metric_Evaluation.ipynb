{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb0b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from text_metrics.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33054e74-b601-4126-97d8-aac532ced2ce",
   "metadata": {},
   "source": [
    "# Helper functions for the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_data(path: str) -> list:\n",
    "    '''\n",
    "    Purpose: Loads in the data from the desired path.\n",
    "    \n",
    "    Args:\n",
    "        path: path to file.\n",
    "        \n",
    "    Outputs:\n",
    "        data: list of dictionaries containing the data.\n",
    "    '''\n",
    "    with open(path, 'r') as f:\n",
    "        data = []\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def average_human_scores(data: list) -> list:\n",
    "    '''\n",
    "    Purpose: Based on the experiments done in (https://arxiv.org/pdf/2105.02573.pdf), we need to average each human rating of synthetic text examples, so we can compare\n",
    "    them to the FBD score accurately. This function takes the human scores (if multiple scores are provided, we take the last score as that should relate to overall score\n",
    "    instead of subscores, such as grammar or semantics) and averages them into a single score.\n",
    "    \n",
    "    Args:\n",
    "        data: The data loaded by the grab_data function.\n",
    "        \n",
    "    Outputs:\n",
    "        human_scores: list of averaged human scores.\n",
    "    '''\n",
    "    num_scores = len(data[0]['human_scores'])\n",
    "    human_scores = []\n",
    "    raw_scores = defaultdict(list)\n",
    "    for line in data:\n",
    "        for i in range(num_scores):\n",
    "            raw_scores[i].append(line['human_scores'][i][-1])\n",
    "\n",
    "    for i in raw_scores.keys():\n",
    "        human_scores.append(sum(raw_scores[i])/len(raw_scores[i]))\n",
    "    \n",
    "    return human_scores\n",
    "\n",
    "def collate_data(data: list) -> (list, list):\n",
    "    '''\n",
    "    Purpose: In the experimentations done in (https://arxiv.org/pdf/2105.02573.pdf), we need to concatenate the queries and responses in order to make the vectors for FBD.\n",
    "    This function serves that exact purpose of concatenating the real query with the synthetic responses and real responses respectively.\n",
    "    \n",
    "    Args:\n",
    "        data: The data loaded by grab_data function.\n",
    "        \n",
    "    Outputs:\n",
    "        real_inputs: A list of the real query + real response combos.\n",
    "        synth_inputs: A list of the real query + synthetic response combos.\n",
    "    '''\n",
    "    num_synth = len(data[0]['hyps'])\n",
    "    real_queries = []\n",
    "    synth_answers = defaultdict(list)\n",
    "    real_answers = []\n",
    "    for line in data:\n",
    "        real_queries.append(line['src'])\n",
    "        real_answers.append(line['refs'])\n",
    "        for i in range(num_synth):\n",
    "            synth_answers[i].append(line['hyps'][i])\n",
    "            \n",
    "    real_inputs = []\n",
    "    synth_inputs = defaultdict(list)\n",
    "    for real_query, real_answer in zip(real_queries, real_answers):\n",
    "        real_inputs.append(real_query + ' ' + real_answer[0])\n",
    "\n",
    "    for i in range(num_synth):\n",
    "        for real_query, synth_answer in zip(real_queries, synth_answers[i]):\n",
    "            synth_inputs[i].append(real_query + ' ' + synth_answer)\n",
    "            \n",
    "    return real_inputs, synth_inputs\n",
    "\n",
    "def metric_collater(real_data: list, synth_data: list, model_name: str) -> (list, list):\n",
    "    '''\n",
    "    Purpose: This function runs the fbd and fcsd scores between the real and synthetic text inputs.\n",
    "    \n",
    "    Args:\n",
    "        real_data: A list of the real inputs.\n",
    "        synth_data: A list of the synthetic inputs.\n",
    "        model_name: The name of the model for the embeddings. This will be utilized by SentenceTransformer\n",
    "    \n",
    "    Outputs:\n",
    "        fbd_scores: list of fbd_scores.\n",
    "        fcsd_scores: list of fcsd_scores.\n",
    "    '''\n",
    "    fcsd_scores = []\n",
    "    fbd_scores = []\n",
    "    for key in synth_data.keys():\n",
    "        real_series = pd.DataFrame(real_data, columns = ['Text'])\n",
    "        synth_series = pd.DataFrame(synth_data[key], columns = ['Text'])\n",
    "        fbd_score, fcsd_score = metrics_run(real_series.Text, synth_series.Text, model_name)\n",
    "        fbd_scores.append(fbd_score)\n",
    "        fcsd_scores.append(fcsd_score)\n",
    "    \n",
    "    return fbd_scores, fcsd_scores\n",
    "\n",
    "def experiments(model_names: list, data_names: list, index_names: list, all_data: list) -> dict:\n",
    "    '''\n",
    "    Purpose: Runs the full experiment. Takes all the model names you want to try from Sentence Transformer, all the data that has been loaded in, the metric names, and a\n",
    "    list of all the data to create the scores. This function is quite rigid, but you can use this code as a template to use for other metrics and data.\n",
    "    \n",
    "    Args:\n",
    "        model_names: These are names that come from the Sentence Transformer library (https://huggingface.co/sentence-transformers). Any and all names should be able to\n",
    "        be used.\n",
    "        data_names: These are names attributed to each data source, so we can keep track of the scores they generate in the output dictionary.\n",
    "        index_names: These are the names of the specific scores we calculate with the associated text metric. This is part of the reason the code is so brittle. It requires\n",
    "        that these only have four names associated with four specific metrics. Refactoring would be required to make this more robust.\n",
    "        all_data: A list of each data source grabbed from the grab_data function.\n",
    "        \n",
    "    Outputs:\n",
    "        corrs: A dictionary of the pearson and spearman correlations generated for each metric, dataset pair.\n",
    "    '''\n",
    "    corrs = {}\n",
    "    for data_name, data in zip(data_names, all_data):\n",
    "        human_scores = average_human_scores(data)\n",
    "        real_data, synth_data = collate_data(data)\n",
    "        corrs[data_name] = {}\n",
    "        for model_name in model_names:\n",
    "            corrs[data_name][model_name] = {}\n",
    "            fbd, fcsd = metric_collater(real_data, synth_data, model_name)\n",
    "            spearman_fbd = abs(spearmanr(fbd, human_scores)[0])\n",
    "            spearman_fcsd = abs(spearmanr(fcsd, human_scores)[0])\n",
    "            pearson_fbd = abs(pearsonr(fbd, human_scores)[0])\n",
    "            pearson_fcsd = abs(pearsonr(fcsd, human_scores)[0])\n",
    "            metrics = [spearman_fbd, spearman_fcsd, pearson_fbd, pearson_fcsd]\n",
    "            for name, metric in zip(index_names, metrics):\n",
    "                corrs[data_name][model_name][name] = metric\n",
    "    \n",
    "    return corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ae3e18-c8f8-4945-add8-42b5617d2504",
   "metadata": {},
   "source": [
    "# Experimentation Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9428882",
   "metadata": {},
   "outputs": [],
   "source": [
    "convai_data = grab_data('datasets/convai2_annotation.json')\n",
    "dailyh_data = grab_data('datasets/dailyh_annotation.json')\n",
    "dailyz_data = grab_data('datasets/dailyz_annotation.json')\n",
    "emp_data = grab_data('datasets/empathetic_annotation.json')\n",
    "personam_data = grab_data('datasets/personam_annotation.json')\n",
    "personaz_data = grab_data('datasets/personaz_annotation.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb04d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "convai_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba936e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyh_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c5f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyz_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc26a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbecc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "personaz_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7003ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'paraphrase-mpnet-base-v2',\n",
    "    'paraphrase-TinyBERT-L6-v2',\n",
    "    'paraphrase-distilroberta-base-v2',\n",
    "    'paraphrase-MiniLM-L12-v2',\n",
    "    'paraphrase-MiniLM-L6-v2',\n",
    "    'paraphrase-albert-small-v2',\n",
    "    'paraphrase-MiniLM-L3-v2',\n",
    "    'nli-mpnet-base-v2',\n",
    "    'stsb-mpnet-base-v2',\n",
    "    'stsb-distilroberta-base-v2',\n",
    "    'nli-roberta-base-v2',\n",
    "    'stsb-roberta-base-v2',\n",
    "    'nli-distilroberta-base-v2',\n",
    "]\n",
    "\n",
    "data_names= [\n",
    "    'convai',\n",
    "    'dailyh',\n",
    "    'dailyz',\n",
    "    'emp',\n",
    "    'personam',\n",
    "    'personaz'\n",
    "]\n",
    "\n",
    "index_names = [\n",
    "    'spearman_fbd',\n",
    "    'spearman_fcsd',\n",
    "    'pearson_fbd',\n",
    "    'pearson_fcsd'\n",
    "]\n",
    "\n",
    "all_data = [\n",
    "    convai_data,\n",
    "    dailyh_data,\n",
    "    dailyz_data,\n",
    "    emp_data,\n",
    "    personam_data,\n",
    "    personaz_data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69842632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrs = experiments(model_names, data_names, index_names, all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98d6656-4a93-4c4d-b49b-fef57d69dce0",
   "metadata": {},
   "source": [
    "# View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43fc29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f227fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(corrs['convai'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(corrs['dailyh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d96c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(corrs['dailyz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8587a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(corrs['emp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(corrs['personaz'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
